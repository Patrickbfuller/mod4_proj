{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn_pandas import DataFrameMapper, cross_val_score\n",
    "from sklearn_pandas.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('prepped_data.json').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>weekday_posted</th>\n",
       "      <th>hour_posted</th>\n",
       "      <th>log_score</th>\n",
       "      <th>log_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300</td>\n",
       "      <td>1588</td>\n",
       "      <td>21720</td>\n",
       "      <td>China Killing Prisoners To Harvest Organs For ...</td>\n",
       "      <td>2019-06-19 11:49:08</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>9.985989</td>\n",
       "      <td>7.370237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>240</td>\n",
       "      <td>402</td>\n",
       "      <td>2661</td>\n",
       "      <td>Muslim family dragged out of Belgian embassy i...</td>\n",
       "      <td>2019-06-19 12:05:31</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>7.886457</td>\n",
       "      <td>5.996477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>660</td>\n",
       "      <td>3320</td>\n",
       "      <td>46977</td>\n",
       "      <td>Women outperform men after Japan medical schoo...</td>\n",
       "      <td>2019-06-19 05:51:44</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>10.757413</td>\n",
       "      <td>8.107723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>360</td>\n",
       "      <td>202</td>\n",
       "      <td>1474</td>\n",
       "      <td>MH17 crash: Investigators 'to charge four with...</td>\n",
       "      <td>2019-06-19 10:50:51</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>7.295735</td>\n",
       "      <td>5.308317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>660</td>\n",
       "      <td>1336</td>\n",
       "      <td>2665</td>\n",
       "      <td>Iranian official calls on world to unite again...</td>\n",
       "      <td>2019-06-19 05:09:15</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>7.887959</td>\n",
       "      <td>7.197443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  num_comments  score  \\\n",
       "0  300          1588  21720   \n",
       "1  240           402   2661   \n",
       "2  660          3320  46977   \n",
       "3  360           202   1474   \n",
       "4  660          1336   2665   \n",
       "\n",
       "                                                text           timestamp  \\\n",
       "0  China Killing Prisoners To Harvest Organs For ... 2019-06-19 11:49:08   \n",
       "1  Muslim family dragged out of Belgian embassy i... 2019-06-19 12:05:31   \n",
       "2  Women outperform men after Japan medical schoo... 2019-06-19 05:51:44   \n",
       "3  MH17 crash: Investigators 'to charge four with... 2019-06-19 10:50:51   \n",
       "4  Iranian official calls on world to unite again... 2019-06-19 05:09:15   \n",
       "\n",
       "   weekday_posted  hour_posted  log_score  log_comments  \n",
       "0               2           11   9.985989      7.370237  \n",
       "1               2           12   7.886457      5.996477  \n",
       "2               2            5  10.757413      8.107723  \n",
       "3               2           10   7.295735      5.308317  \n",
       "4               2            5   7.887959      7.197443  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['text','age','weekday_posted','hour_posted']]\n",
    "y = df['log_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2019)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>age</th>\n",
       "      <th>weekday_posted</th>\n",
       "      <th>hour_posted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4867</th>\n",
       "      <td>Donald Trump Jr. on Sunday claimed CNN is cove...</td>\n",
       "      <td>1440</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>Oxfam warns of the \"worst cholera outbreak in ...</td>\n",
       "      <td>1440</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2819</th>\n",
       "      <td>Exposure to weed killing products increases ri...</td>\n",
       "      <td>1440</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>Hong Kong protesters demand China be held to a...</td>\n",
       "      <td>660</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>Austrian Government Seeks to Eliminate Interne...</td>\n",
       "      <td>180</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text   age  weekday_posted  \\\n",
       "4867  Donald Trump Jr. on Sunday claimed CNN is cove...  1440               0   \n",
       "1395  Oxfam warns of the \"worst cholera outbreak in ...  1440               5   \n",
       "2819  Exposure to weed killing products increases ri...  1440               3   \n",
       "567   Hong Kong protesters demand China be held to a...   660               0   \n",
       "1444  Austrian Government Seeks to Eliminate Interne...   180               4   \n",
       "\n",
       "      hour_posted  \n",
       "4867            4  \n",
       "1395           11  \n",
       "2819           11  \n",
       "567             0  \n",
       "1444           10  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Embeddings from GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = X_train.text\n",
    "data = headers.map(word_tokenize).values\n",
    "total_vocabulary = set(word for headline in data for word in headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = {}\n",
    "with open('/Users/patrickfuller/flatiron/glove/glove.6B.50d.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in total_vocabulary:\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LETS BORROW SOME LOVELY CODE FROM OUR CURRICULUM!\n",
    "from webapp import vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'webapp.vectorizer' from '/Users/patrickfuller/flatiron/4_mod/mod4_proj/webapp/vectorizer.py'>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imp import reload\n",
    "reload(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = vectorizer.W2vVectorizer(glove)\n",
    "ohe = OneHotEncoder(drop='first',categories='auto')\n",
    "ss = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = DataFrameMapper([\n",
    "    ('text', w2v),\n",
    "    (['age'], ss),\n",
    "    (['weekday_posted', 'hour_posted'], ohe),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(n_jobs=-1)\n",
    "rfr = RandomForestRegressor(n_jobs=1, n_estimators=100)\n",
    "gbr = GradientBoostingRegressor(n_estimators=100)\n",
    "knnr = KNeighborsRegressor(n_jobs=-1)\n",
    "regressors = [lr,rfr,gbr,knnr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE of the LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, normalize=False)\n",
      "\tis:6790.891015102855\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-634543123738>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m'regressor'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregressor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     ])\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     print(f'The MAE of the {str(regressor)}'\n",
      "\u001b[0;32m/Applications/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    354\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    328\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 330\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/learn-env/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/learn-env/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/learn-env/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/learn-env/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/learn-env/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/learn-env/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/learn-env/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1155\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1157\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    378\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for regressor in regressors:\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('transform',mapper),\n",
    "        ('regressor', regressor)\n",
    "    ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds = np.exp(pipe.predict(X_test))\n",
    "    print(f'The MAE of the {str(regressor)}'\n",
    "        f'\\n\\tis:{mean_absolute_error(np.exp(y_test),preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low model for score prediction seems to be GBT when using GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = df['log_comments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y2_train, y2_test = train_test_split(X, y2, random_state=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for regressor in regressors:\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('transform',mapper),\n",
    "        ('regressor', regressor)\n",
    "    ])\n",
    "    pipe.fit(X_train, y2_train)\n",
    "    preds = np.exp(pipe.predict(X_test))\n",
    "    print(f'The MAE of the {str(regressor)}'\n",
    "        f'\\n\\tis:{mean_absolute_error(np.exp(y2_test),preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Low model for comment prediction seems to be GBT as well when using GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Could using TFIDF in conjunction with GloVe reduce model error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=2000)\n",
    "\n",
    "mapper = DataFrameMapper([\n",
    "    ('text', w2v),\n",
    "    ('text', tfidf),       # Adding a second feature extraction on text\n",
    "    (['age'], ss),\n",
    "    (['weekday_posted', 'hour_posted'], ohe),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate GloVe + TFIDF model error for score prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for regressor in regressors:\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('transform',mapper),\n",
    "        ('regressor', regressor)\n",
    "    ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds = np.exp(pipe.predict(X_test))\n",
    "    print(f'The MAE of the {str(regressor)}'\n",
    "        f'\\n\\tis:{mean_absolute_error(np.exp(y_test),preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate GloVe + TFIDF model error for comment prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for regressor in regressors:\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('transform',mapper),\n",
    "        ('regressor', regressor)\n",
    "    ])\n",
    "    pipe.fit(X_train, y2_train)\n",
    "    preds = np.exp(pipe.predict(X_test))\n",
    "    print(f'The MAE of the {str(regressor)}'\n",
    "        f'\\n\\tis:{mean_absolute_error(np.exp(y2_test),preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A new low of 532: GBT GloVe + TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Could adding features to TFIDF reduce model error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=4000)\n",
    "\n",
    "mapper = DataFrameMapper([\n",
    "    ('text', w2v),\n",
    "    ('text', tfidf),       # Adding a second feature extraction on text\n",
    "    (['age'], ss),\n",
    "    (['weekday_posted', 'hour_posted'], ohe),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for regressor in regressors:\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('transform',mapper),\n",
    "        ('regressor', regressor)\n",
    "    ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds = np.exp(pipe.predict(X_test))\n",
    "    print(f'The MAE of the {str(regressor)}'\n",
    "        f'\\n\\tis:{mean_absolute_error(np.exp(y_test),preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doesn't look like adding features to tfidf helps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets pickle our best performers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score_predictor\n",
    "log_score_mapper = DataFrameMapper([\n",
    "    ('text', w2v),\n",
    "    (['age'], ss),\n",
    "    (['weekday_posted', 'hour_posted'], ohe),\n",
    "    ])\n",
    "log_score_model = Pipeline(steps=[\n",
    "        ('transform', log_score_mapper),\n",
    "        ('regressor', gbr)\n",
    "    ])\n",
    "\n",
    "# Comment_predictor\n",
    "tfidf_c = TfidfVectorizer(max_features=2000)\n",
    "w2v_c = vectorizer.W2vVectorizer(glove)\n",
    "ohe_c = OneHotEncoder(drop='first',categories='auto')\n",
    "ss_c = StandardScaler()\n",
    "\n",
    "gbr_c = GradientBoostingRegressor()\n",
    "\n",
    "log_comment_mapper = DataFrameMapper([\n",
    "    ('text', w2v_c),\n",
    "    ('text', tfidf_c),       # Adding a second feature extraction on text\n",
    "    (['age'], ss_c),\n",
    "    (['weekday_posted', 'hour_posted'], ohe_c),\n",
    "    ])\n",
    "log_comment_model = Pipeline(steps=[\n",
    "        ('transform', log_comment_mapper),\n",
    "        ('regressor', gbr_c)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('transform',\n",
       "                 DataFrameMapper(default=False, df_out=False,\n",
       "                                 features=[('text',\n",
       "                                            W2vVectorizer(w2v={'!': array([-0.58402 ,  0.39031 ,  0.65282 , -0.3403  ,  0.19493 , -0.83489 ,\n",
       "        0.11929 , -0.57291 , -0.56844 ,  0.72989 , -0.56975 ,  0.53436 ,\n",
       "       -0.38034 ,  0.22471 ,  0.98031 , -0.2966  ,  0.126   ,  0.55222 ,\n",
       "       -0.62737 , -0.082242, -0.085359,  0.31515 ,  0.96077 ,  0.31986 ,\n",
       "        0.87878 , -1.5189  , -...\n",
       "                                           init=None, learning_rate=0.1,\n",
       "                                           loss='ls', max_depth=3,\n",
       "                                           max_features=None,\n",
       "                                           max_leaf_nodes=None,\n",
       "                                           min_impurity_decrease=0.0,\n",
       "                                           min_impurity_split=None,\n",
       "                                           min_samples_leaf=1,\n",
       "                                           min_samples_split=2,\n",
       "                                           min_weight_fraction_leaf=0.0,\n",
       "                                           n_estimators=40,\n",
       "                                           n_iter_no_change=None,\n",
       "                                           presort='auto', random_state=None,\n",
       "                                           subsample=1.0, tol=0.0001,\n",
       "                                           validation_fraction=0.1, verbose=0,\n",
       "                                           warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_score_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.72596028])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_score_model.predict(X_test[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('transform',\n",
       "                 DataFrameMapper(default=False, df_out=False,\n",
       "                                 features=[('text',\n",
       "                                            W2vVectorizer(w2v={'!': array([-0.58402 ,  0.39031 ,  0.65282 , -0.3403  ,  0.19493 , -0.83489 ,\n",
       "        0.11929 , -0.57291 , -0.56844 ,  0.72989 , -0.56975 ,  0.53436 ,\n",
       "       -0.38034 ,  0.22471 ,  0.98031 , -0.2966  ,  0.126   ,  0.55222 ,\n",
       "       -0.62737 , -0.082242, -0.085359,  0.31515 ,  0.96077 ,  0.31986 ,\n",
       "        0.87878 , -1.5189  , -...\n",
       "                                           init=None, learning_rate=0.1,\n",
       "                                           loss='ls', max_depth=3,\n",
       "                                           max_features=None,\n",
       "                                           max_leaf_nodes=None,\n",
       "                                           min_impurity_decrease=0.0,\n",
       "                                           min_impurity_split=None,\n",
       "                                           min_samples_leaf=1,\n",
       "                                           min_samples_split=2,\n",
       "                                           min_weight_fraction_leaf=0.0,\n",
       "                                           n_estimators=100,\n",
       "                                           n_iter_no_change=None,\n",
       "                                           presort='auto', random_state=None,\n",
       "                                           subsample=1.0, tol=0.0001,\n",
       "                                           validation_fraction=0.1, verbose=0,\n",
       "                                           warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_comment_model.fit(X_train, y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.34227749])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_comment_model.predict(X_test[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>age</th>\n",
       "      <th>weekday_posted</th>\n",
       "      <th>hour_posted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4867</th>\n",
       "      <td>Donald Trump Jr. on Sunday claimed CNN is cove...</td>\n",
       "      <td>1440</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text   age  weekday_posted  \\\n",
       "4867  Donald Trump Jr. on Sunday claimed CNN is cove...  1440               0   \n",
       "\n",
       "      hour_posted  \n",
       "4867            4  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>age</th>\n",
       "      <th>weekday_posted</th>\n",
       "      <th>hour_posted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Korea</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 text  age  weekday_posted  hour_posted\n",
       "0  Donald Trump Korea   12               1           18"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age = 12\n",
    "weekday = 1\n",
    "hour = 18\n",
    "text = 'Donald Trump Korea'\n",
    "\n",
    "arguments = pd.DataFrame([[text, age, weekday, hour]], columns=['text', 'age', 'weekday_posted', 'hour_posted'])\n",
    "arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.30917486])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_score_model.predict(arguments[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.35623333])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_comment_model.predict(arguments[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('log_score_model.pkl', 'wb') as f:\n",
    "    pickle.dump(log_score_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('log_comment_model.pkl', 'wb') as f:\n",
    "    pickle.dump(log_comment_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "from webapp import vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FunctionTransformer(lambda x: x).fit(X_train.text)     # Function Transformer can not handle text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextTransformer()"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_transformer = vectorizer.TextTransformer()\n",
    "text_transformer.fit(X_train.text, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([list(['Donald', 'Trump', 'Jr.', 'on', 'Sunday', 'claimed', 'CNN', 'is', 'covering', 'up', 'for', '``', 'leftist', 'hack', \"''\", 'Carl', 'Bernstein', ',', 'a', 'veteran', 'journalist', 'known', 'for', 'his', 'coverage', 'of', 'Watergate', ',', 'over', 'his', 'reporting', 'about', 'the', '2016', 'Trump', 'Tower', 'meeting', '.']),\n",
       "       list(['Oxfam', 'warns', 'of', 'the', '``', 'worst', 'cholera', 'outbreak', 'in', 'the', 'world', \"''\", 'in', 'Yemen']),\n",
       "       list(['Exposure', 'to', 'weed', 'killing', 'products', 'increases', 'risk', 'of', 'cancer', 'by', '41', '%', ',', 'finds', 'a', 'new', 'study', 'that', 'provides', 'evidence', 'that', '‘', 'supports', 'link', '’', 'between', 'exposures', 'to', 'glyphosate', 'herbicides', 'and', 'increased', 'risk', 'for', 'non-Hodgkin', 'lymphoma', '.']),\n",
       "       list(['Hong', 'Kong', 'protesters', 'demand', 'China', 'be', 'held', 'to', 'account', 'for', '1989', 'Tiananmen', 'Massacre', '.']),\n",
       "       list(['Austrian', 'Government', 'Seeks', 'to', 'Eliminate', 'Internet', 'Anonymity', '–', 'With', 'Severe', 'Penalties'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_transformer.transform(X_train.text)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(n_estimators=40)\n",
    "text_transformer = vectorizer.TextTransformer()\n",
    "w2v = vectorizer.W2vVectorizer(glove)\n",
    "text_pipe = Pipeline(steps=[\n",
    "    ('tok', text_transformer),\n",
    "    ('w2v', w2v)\n",
    "])\n",
    "\n",
    "log_score_mapper = DataFrameMapper([\n",
    "    ('text', text_pipe),\n",
    "    (['age'], ss),\n",
    "    (['weekday_posted', 'hour_posted'], ohe),\n",
    "    ])\n",
    "log_score_model = Pipeline(steps=[\n",
    "        ('transform', log_score_mapper),\n",
    "        ('regressor', gbr)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('transform',\n",
       "                 DataFrameMapper(default=False, df_out=False,\n",
       "                                 features=[('text',\n",
       "                                            Pipeline(memory=None,\n",
       "                                                     steps=[('tok',\n",
       "                                                             TextTransformer()),\n",
       "                                                            ('w2v',\n",
       "                                                             W2vVectorizer(w2v={'!': array([-0.58402 ,  0.39031 ,  0.65282 , -0.3403  ,  0.19493 , -0.83489 ,\n",
       "        0.11929 , -0.57291 , -0.56844 ,  0.72989 , -0.56975 ,  0.53436 ,\n",
       "       -0.38034 ,  0.22471 ,  0.98031 , -0.2966  ,  0.126   ,  0.55222 ,\n",
       "       -0.62737 ,...\n",
       "                                           init=None, learning_rate=0.1,\n",
       "                                           loss='ls', max_depth=3,\n",
       "                                           max_features=None,\n",
       "                                           max_leaf_nodes=None,\n",
       "                                           min_impurity_decrease=0.0,\n",
       "                                           min_impurity_split=None,\n",
       "                                           min_samples_leaf=1,\n",
       "                                           min_samples_split=2,\n",
       "                                           min_weight_fraction_leaf=0.0,\n",
       "                                           n_estimators=40,\n",
       "                                           n_iter_no_change=None,\n",
       "                                           presort='auto', random_state=None,\n",
       "                                           subsample=1.0, tol=0.0001,\n",
       "                                           validation_fraction=0.1, verbose=0,\n",
       "                                           warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_score_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "The MAE of the RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "                      max_features='auto', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "                      oob_score=False, random_state=None, verbose=0,\n",
      "                      warm_start=False)\n",
      "\tis:6678.364767414466\n"
     ]
    }
   ],
   "source": [
    "preds = np.exp(log_score_model.predict(X_test))\n",
    "print(f'The MAE of the {str(regressor)}'\n",
    "        f'\\n\\tis:{mean_absolute_error(np.exp(y_test),preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Didn't do much better hmm\n",
    "### maybe on comments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment_predictor\n",
    "tfidf_c = TfidfVectorizer(max_features=2000)\n",
    "w2v_c = vectorizer.W2vVectorizer(glove)\n",
    "ohe_c = OneHotEncoder(drop='first',categories='auto')\n",
    "ss_c = StandardScaler()\n",
    "\n",
    "text_transformer_c = vectorizer.TextTransformer()\n",
    "text_pipe_c = Pipeline(steps=[\n",
    "    ('tok', text_transformer_c),\n",
    "    ('w2v', w2v_c)\n",
    "])\n",
    "\n",
    "gbr_c = GradientBoostingRegressor(n_estimators=30)\n",
    "\n",
    "log_comment_mapper = DataFrameMapper([\n",
    "    ('text', text_pipe_c),\n",
    "    ('text', tfidf_c),       # Adding a second feature extraction on text\n",
    "    (['age'], ss_c),\n",
    "    (['weekday_posted', 'hour_posted'], ohe_c),\n",
    "    ])\n",
    "log_comment_model = Pipeline(steps=[\n",
    "        ('transform', log_comment_mapper),\n",
    "        ('regressor', gbr_c)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('transform',\n",
       "                 DataFrameMapper(default=False, df_out=False,\n",
       "                                 features=[('text',\n",
       "                                            Pipeline(memory=None,\n",
       "                                                     steps=[('tok',\n",
       "                                                             TextTransformer()),\n",
       "                                                            ('w2v',\n",
       "                                                             W2vVectorizer(w2v={'!': array([-0.58402 ,  0.39031 ,  0.65282 , -0.3403  ,  0.19493 , -0.83489 ,\n",
       "        0.11929 , -0.57291 , -0.56844 ,  0.72989 , -0.56975 ,  0.53436 ,\n",
       "       -0.38034 ,  0.22471 ,  0.98031 , -0.2966  ,  0.126   ,  0.55222 ,\n",
       "       -0.62737 ,...\n",
       "                                           init=None, learning_rate=0.1,\n",
       "                                           loss='ls', max_depth=3,\n",
       "                                           max_features=None,\n",
       "                                           max_leaf_nodes=None,\n",
       "                                           min_impurity_decrease=0.0,\n",
       "                                           min_impurity_split=None,\n",
       "                                           min_samples_leaf=1,\n",
       "                                           min_samples_split=2,\n",
       "                                           min_weight_fraction_leaf=0.0,\n",
       "                                           n_estimators=30,\n",
       "                                           n_iter_no_change=None,\n",
       "                                           presort='auto', random_state=None,\n",
       "                                           subsample=1.0, tol=0.0001,\n",
       "                                           validation_fraction=0.1, verbose=0,\n",
       "                                           warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_comment_model.fit(X_train, y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3631x2000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 48962 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_c.fit_transform(X_train.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
